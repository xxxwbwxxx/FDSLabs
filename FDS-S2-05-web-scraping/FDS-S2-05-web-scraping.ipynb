{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Science\n",
    "## Semester 2 Week 05: Getting data and web scraping\n",
    "\n",
    "**Learning outcomes:** In this lab you will learn about getting data via web scraping. By the end of this lab you should be able to:\n",
    "\n",
    "- Identify when you need to use web scraping to retrieve data\n",
    "- Determine whether a proposed web scraping exercise is legal\n",
    "- Use the Python `requests` module to retrieve a web page in a responsible fashion\n",
    "- Use the Python `BeautifulSoup` package to isolate and retrieve data from a HTML web page\n",
    "- Understand how web scraping can be automated\n",
    "\n",
    "This lab is somewhat inspired by this one \n",
    "https://ourcodingclub.github.io/tutorials/webscraping/ .\n",
    "\n",
    "This lab is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n",
    "\n",
    "You might wish to review the contents of the S1 week 4 lecture before or after this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "# Package to display the hints and soultions\n",
    "from common.show_solutions import show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. When do we need to do web scraping to retrieve data?\n",
    "\n",
    "Web scraping is the process of automating the process of extracting data from a web page.\n",
    "\n",
    "Spring is in the air (in Edinburgh at least), so we might be inspired to find out more about the birds around us. We'll imagine you want to collect information on the characteristics of UK birds, and put them in a table like this:\n",
    "\n",
    "| Species            | Length   | Wingspan    | Weight                         | UK passage   | Europe          | UK breeding   |\n",
    "|:-------------------|:---------|:------------|:-------------------------------|:-------------|:----------------|:--------------|\n",
    "| Aquatic warbler    | 13cm     | 16.5-19.5cm | 10-14g                         | 42 birds     | 12-20,000 pairs | nan           |\n",
    "| White-tailed eagle | 70-90cm  | 200-240cm   | 3.5-5kg (male); 4-7kg (female) | nan          | nan             | 106 pairs     |\n",
    "\n",
    "\n",
    "It’s easy enough to head to the [Royal Society for the Protection of Birds (RSPB) A-Z listing of birds](https://www.rspb.org.uk/birds-and-wildlife/wildlife-guides/bird-a-z/), click through each page, then copy the relevant information and paste it into a spreadsheet. Now imagine you want to repeat this for all 238 birds! This can quickly become very tedious as you click between lots of pages, repeating the same actions over and over again. It also increases the chance of making mistakes when copying and pasting. By automating this process, i.e. web scraping, you can reduce the chance of making mistakes and speed up your data collection. Additionally, once you have written the script, it can be adapted for a lot of different projects, saving time in the long run.\n",
    "\n",
    "The first rule of web scraping is **don't do it unless you have to**. Data is increasingly available in structured formats such as CSV and JSON. Web scraping seems cool, but it's also time-consuming and fiddly. It pays to spend some time searching for files in structured formats, in which the data may well be in a cleaner format than on a web page. However, if you can't find the data you need in a structured format, but it is available on a web page, then web scraping can be helpful.\n",
    "\n",
    "## B. Before we start - the law and ethics of web-scraping\n",
    "\n",
    "It is worth remembering that web scraping involves a relationship between you, the web scraper, and the owner of the website, who may have invested considerable time and money in creating their site. It's possible that by web scraping you could have an adverse effect on either their intellectual property or their web server, so we need to think a little about law and ethics.\n",
    "\n",
    "The law around web scraping is not always clear ([Byrce Davies - _Is web scraping legal?_](https://scrapediary.com/is-web-scraping-legal/)) but you should always check the terms and conditions of the website before before starting scraping. For example, [the Copyright Policy of the Financial Times website](https://help.ft.com/legal-privacy/copyright-policy/) says you cannot\n",
    "> \"Frame, harvest or scrape FT content or otherwise access FT content for similar purposes.\"\n",
    "\n",
    "Beyond the legal restrictions for a specific website, we should also consider general ethical principles of web scraping. James Densmore, in his article [*Ethics in webscraping*](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01), suggests the following set rules for those undertaking web scraping:\n",
    "\n",
    "> - \"If you have a public API that provides the data I’m looking for, I’ll use it and avoid scraping all together.\n",
    "> - I will always provide a User Agent<sup>*</sup> string that makes my intentions clear and provides a way for you to contact me with questions or concerns.\n",
    "> - I will request data at a reasonable rate. I will strive to never be confused for a [DDoS](https://en.wikipedia.org/wiki/Denial-of-service_attack) attack.\n",
    "> - I will only save the data I absolutely need from your page. [...]\n",
    "> - I will respect any content I do keep. I’ll never pass it off as my own.\n",
    "> - I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
    "> - I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
    "> - I will scrape for the purpose of creating new value from the data, not to duplicate it.\"\n",
    "\n",
    "<sup>*</sup> We'll explain the meaning of \"User Agent\" later.\n",
    "\n",
    "Equipped with this information, we'll now start scraping.\n",
    "\n",
    "### B.1 First check the copyright\n",
    "\n",
    "**Exercise 01:** Take a look at the RSPB website [Copyright information](https://www.rspb.org.uk/help/copyright) and [Terms and Conditions](https://www.rspb.org.uk/help/terms--conditions/). Can you see anything that would prevent us scraping the information? (Note that education is classed as a non-commercial activity.) What restrictions are there on how you can use the information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Requesting robots.txt\n",
    "\n",
    "A web server is just another computer on the internet that is listening for **requests** from other computers on the internet. When we're browsing the web, our web browsers are making requests to web servers all around the internet. The web browser is one example of a **user agent**. We're now going to use the python `requests` package as a user agent.\n",
    "\n",
    "The first file we will retrieve is called `robots.txt`.\n",
    "\n",
    "Every web server should have this file to tell robots (i.e. automated user agents like search engine crawlers that move from page to page on the web) what the website owner is happy for them to request. (Note that the file is purely advisory. Bad bots - malware etc - will ignore this file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rspb.org.uk/robots.txt'\n",
    "r = requests.get(url)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`r` is a request object. `r.text` contains the contents of the `robots.txt` file.\n",
    "\n",
    "The first line of the file `User-agent: *` tells us that the next section of the file applies to *all* user agents - that includes us! The next lines tell us the areas of the site that should not be scraped automatically. The RSPB don't want us scraping any pages whose URLs start https://www.rspb.org.uk/search ,\n",
    "https://www.rspb.org.uk/community/SiteMap.ashx etc. There is more information on the format of the `robots.txt` file [here](https://www.robotstxt.org/robotstxt.html).\n",
    "\n",
    "Note that the web server will have a record of the \"User-agent\" in its logs. For example, the request we've just made will be recorded in the web server logs like this:\n",
    "```\n",
    "77.99.216.20 - - [20/Feb/2021:18:24:40 +0000] \"GET /robots.txt HTTP/1.1\" 200 4594 \"-\" \"python-requests/2.24.0\"\n",
    "```\n",
    "This means that a user agent called \"python-requests/2.24.0\" has made a request from the IP address 77.99.216.20 for the file `robots.txt`. Following the suggested guidelines on ethical web scraping, it would be polite to advertise who we are, by changing the name of the user agent in the request to something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Foundations of Data Science Course (https://www.github.com/Inf2-FDS/weekS2-06-web-scraping)'}\n",
    "r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This request will appear in the server logs like this:\n",
    "```\n",
    "77.99.216.20 - - [20/Feb/2021:19:57:16 +0000] \"GET /robots.txt HTTP/1.1\" 200 4594 \"-\" \"Foundations of Data Science Course (https://www.github.com/Inf2-FDS/weekS2-06-web-scraping)\"\n",
    "```\n",
    "If the site owner gets annoyed by frequent requests, they can at least let us know about the problem. But please, when you web scrape, \"request data at a reasonable rate\" (as per the ethical guidelines above). I.e. don't make lots of requests in quick succession (see later on how to do this).\n",
    "\n",
    "Note that the server log also includes the Status code 200 - this means that the request has been delivered successfully by the server. At our end we can check that the code is 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code starts with 2, it means the request was successful. If it starts with 3, it means that the request was successful. If it starts with 4, it means that the page doesn't exist, and if it starts with 5, it indicates an error at the server end. For full information on all status codes, see the [Mozilla developer documentation's page on status codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status).\n",
    "\n",
    "**Exercise 02:** Make a request to the non-existent page https://www.rspb.org.uk/dodo and report on the error code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Getting our first bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 03:** Get the RSPB web page for the White-tailed eagle:\n",
    "https://www.rspb.org.uk/birds-and-wildlife/wildlife-guides/bird-a-z/white-tailed-eagle/ and print the text of the page that you have retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1 Quick review of HTML and some useful terminology\n",
    "\n",
    "What you should see is the Hypertext Markup Language (HTML) source of the page. It's exactly what you would see if you looked at the page source in the browser.\n",
    "\n",
    "You probably know already, but an HTML document has a nested structure, like this:\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Page title</title>\n",
    "    <script src='https://www.googletagmanager.com/gtag/js?id=UA-11409245-4'/>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Aquatic warbler</h1>\n",
    "    <p style='color: red'>Stuff about the aquatic warbler</p>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "The angle brackets containing text are called [tags](https://developer.mozilla.org/en-US/docs/Glossary/Tag). Tags either:\n",
    "- come in pairs, with an opening tag and a closing tag, e.g. `<h1>...</h1>`\n",
    "- or they may be a single tag, in which case they have  slash to indicate that they are a closing tag: e.g. `<script ... />`\n",
    "\n",
    "Some tags have [attributes](https://developer.mozilla.org/en-US/docs/Glossary/Attribute), such as the `style` attribute in the opening `<p>` tag and the `src` attribute in the `<script>` tag.\n",
    "\n",
    "*Everything* between a pair of tags is an [element](https://developer.mozilla.org/en-US/docs/Glossary/Element). For example, the `<html>` element is all the text (apart from the `DOCTYPE`). The `<head>` element contains metadata, while the `<body>` element contains text that is visible on the website:\n",
    "```\n",
    "<body>\n",
    "  <h1>Aquatic warbler</h1>\n",
    "  <p style='color: red'>Stuff about the aquatic warbler</p>\n",
    "</body>\n",
    "```\n",
    "\n",
    "The indentation structure in the code indicates that there is a family tree-like structure to the document. We call the `<html>` the root element. It has two children (`<head>` and `<body`>). In turn `<body>` has two children: `<h1>` and `<p>`. Because `<h1>` and `<p>` have the same parent, they are [siblings](https://dictionary.cambridge.org/us/dictionary/english/sibling).\n",
    "\n",
    "### C.2 Making BeautifulSoup from the bird page\n",
    "\n",
    "You can just see some tags in the text returned by our request, but the tree structure is not clear - we just have a string of text. Fortunately there is a Python package called [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) that will help use to _parse_ the text so that we can extract the tags and their contents (i.e. the elements) easily.\n",
    "\n",
    "**Note:** Crazy name, crazy package. The name BeautifulSoup relates to an episode in an equally crazy book, [*Alice in Wonderland*](http://www.gutenberg.org/ebooks/11) by the mathematician Lewis Carroll, which features a number of birds, including an Eaglet (a baby eagle). Take a minute to look at the video of the White tailed eaglet being fed by its parents on the page we are scraping: https://www.rspb.org.uk/birds-and-wildlife/wildlife-guides/bird-a-z/white-tailed-eagle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that HTML code is printed in a somewhat prettier way. In fact BeatifulSoup has reconstructed the parse tree of the document.\n",
    "\n",
    "### C.3 Accessing tags and elements\n",
    "\n",
    "We can access the first tag of a particular type using the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The title element\n",
    "print(soup.title)\n",
    "\n",
    "## The name of the tag\n",
    "print(soup.title.name)\n",
    "\n",
    "## The contents of the tag\n",
    "print(soup.title.string)\n",
    "\n",
    "## An img (Image) tag\n",
    "print(soup.img)\n",
    "\n",
    "## An a (Anchor) tag\n",
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the last tag we have retrieved is an anchor (i.e. link). It has an attribute `href` containing the target, for example a URL. We can extract the contents of the `href` attribute like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.img['alt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 04:** Get the contents of the first `h1` tag. (`h1` stands for \"Heading text level 1\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you've now got the name of the bird, with one line of code.\n",
    "\n",
    "### C.4 Navigating deeper into the tree\n",
    "\n",
    "Now to find the length, the wingspan, the weight and the number of UK breeding pairs of the White tailed eagle.\n",
    "\n",
    "We can navigate down the tree structure like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the first `div` element that is the child of the `body` element\n",
    "soup.body.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 05:** Have a go at navigating the tree by extending the syntax above, e.g.:\n",
    "```\n",
    "soup.body.div.div.div.div.div\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it's going to be tricky navigating a tree like this.\n",
    "\n",
    "### C.5 Finding the measurements\n",
    "\n",
    "Probably the trickiest part of web scraping, is finding the elements to extract, in this case the measurements. One way to do this is to use the \"Inspector\" function in your browser. In Firefox you can do this by pressing `Ctrl-Shift-I` - other browsers have similar developer tools. Then, as in the picture below:\n",
    "\n",
    "1. Select the \"element picker\"\n",
    "2. Click on the part of the page you are interested in (we'll call this the element of interest)\n",
    "3. Look at the corresponding code. You can copy it by right-clicking on it and selecting **Copy->Outer HTML**.\n",
    "\n",
    "![Screenshot of Inspector in Firefox](src/Screenshot-inspector-annotated.png)\n",
    "\n",
    "The HTML we have found looks like this:\n",
    "```\n",
    "<div class=\"species-measurements-population__measurements\">\n",
    "    <h3 class=\"key-info-title-measurements\">Measurements:</h3>\n",
    "    <dl class=\"species-measurements-population__details\">\n",
    "        <dt class=\"species-measurements-population__details-label\">Length:</dt>\n",
    "        <dd class=\"species-measurements-population__details-content\">\n",
    "            13cm\n",
    "        </dd>\n",
    "    </dl>\n",
    "\n",
    "    <dl class=\"species-measurements-population__details\">\n",
    "        <dt class=\"species-measurements-population__details-label\">Wingspan:</dt>\n",
    "        <dd class=\"species-measurements-population__details-content\">\n",
    "            16.5-19.5cm\n",
    "        </dd>\n",
    "    </dl>\n",
    "\n",
    "    <dl class=\"species-measurements-population__details\">\n",
    "        <dt class=\"species-measurements-population__details-label\">Weight:</dt>\n",
    "        <dd class=\"species-measurements-population__details-content\">\n",
    "            10-14g\n",
    "        </dd>\n",
    "   </dl>\n",
    "</div>\n",
    "```\n",
    "\n",
    "We now need to work out how to retrieve these elements. Maybe if we could get all of the `<dl>`  (\"description list\") elements, we would pick up the measurements we want, and no other `<dl>` elements in the document.\n",
    "\n",
    "### C.6 The `find_all()` method\n",
    "\n",
    "So far, we've been able to find the first instance of any element in the tree, but we want all three `<dl>` elements. More often than not the element or elements we want won't be the first occurrence in the document. Fortunately BeautifulSoup offers other methods of navigating the tree.  If we want to extract all the `<dl>` elements we can use the `find_all` function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.find_all('dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks more promising. We now have a list of elements that we can iterate over, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = soup.body.find_all('dl')\n",
    "for measurement in measurements:\n",
    "    print(measurement.dt.string)\n",
    "    print(measurement.dd.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a lot of white space in what we have scraped. How do we get rid of it?\n",
    "\n",
    "### C. Enter regular expressions\n",
    "\n",
    "You encountered [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) (also referred to as **regexs**) in the *Data wrangling II* lab. To recap, regexs are sets of character combinations that Python (and many other programming languages) can understand and that are used to search through character strings. We will demonstrate how to use the python `re` package to remove whitespace.\n",
    "\n",
    "The `re.sub` function replaces the parts of a string matching a pattern with a desired replacement. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(\"^\\\\s+|\\\\s+$\", '', measurements[0].dd.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let’s break down `^\\\\s+|\\\\s+$` to understand what it means:\n",
    "\n",
    "`^` = From start of string\n",
    "\n",
    "`\\\\s` = White space\n",
    "\n",
    "`+` = Select 1 or more instances of the character before\n",
    "\n",
    "`|` = “or”\n",
    "\n",
    "`$` = To the end of the line\n",
    "\n",
    "So `^\\\\s+|\\\\s+$` can be interpreted as \"Select one or more white spaces that exist at the start of the string and select one or more white spaces that exist at the end of the string\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know everything we need to know to extract the data from the page and create a pandas DataFrame containing the following information:\n",
    "\n",
    "- Species of bird\n",
    "- All measurements associated with the bird\n",
    "\n",
    "### D.1 Text to DataFrame\n",
    "\n",
    "**Exercise 06:** Create a function `get_measurements` that takes the text of a bird species page on the RSPB website (i.e. `r.text`, and returns a Pandas DataFrame with the above information. Make sure that any whitespace has been removed. Apply the function on the text above and store the DataFrame as `eagle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.2 Joining DataFrames\n",
    "\n",
    "**Exercise 07:** Use your function to get the corresponding DataFrame for the Aquatic warbler: https://www.rspb.org.uk/birds-and-wildlife/wildlife-guides/bird-a-z/aquatic-warbler/ . We suggest you store the data frame in a variable `warbler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the set of measurements for the aquatic warbler is not the same as for the eagle - e.g we don't have the `UK breeding` measurement, but we do have a `Europe` measurement.\n",
    "\n",
    "**Exercise 08:** Create a unified table from the `eagle` and `warbler` DataFrames. If a measurement doesn't exist for one bird, but does for the other, the non-existent measurement should be set to `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to be offered with hints and solution\n",
    "show(question=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.3 Scraping multiple pages\n",
    "\n",
    "We've now finished the practical part of this lab, but here is how we would proceed if we wanted to automate the scraping of more birds from the RSPB website. To do this we would:\n",
    "- Get the contents of the A-Z listing: https://www.rspb.org.uk/birds-and-wildlife/wildlife-guides/bird-a-z/   (The `robots.txt` file says this is OK.)\n",
    "- Find the links to the pages with birds which we will find by searching for `<a>` tags and extracting the `href` attributes.\n",
    "- Create an empty data frame\n",
    "- For each page:\n",
    "  - Request the text of the page\n",
    "  - Run the function that we've just written\n",
    "  - Store the series in the data frame\n",
    "  - Run `time.sleep(10)` - this means that there is a pause of 10 seconds before we make the next request. This will reduce the load on the RSPB website, and prevent them from thinking that we are trying to mount a DDoS attack. Web servers can be configured to detect rapid requests in succession and can then block further requests.\n",
    "  \n",
    "It gets more complicated, because this there are only 16 birds listed on each A-Z page, and you need to find a way of iterating through each listing page. There is no need to undertake this exercise. If you have to do web scraping you will find that every website is structured differently, and you will need to find a strategy that works for that site.\n",
    "\n",
    "It is also worth remembering that sites change their structure quite often, so if you programmed a scraper for a site last year, it may not work this year.\n",
    "\n",
    "**Exercise 09: (Optional)** Implement the above procedure for one page worth of birds on the RSPB website.\n",
    "\n",
    "### D.4 Cleaning the data\n",
    "\n",
    "Although we have the measurements that we wanted, it will be hard to analyse them numerically, since they are very unstructured. For example:\n",
    "- We have **Wingspans** of \"16.5-19.5cm\" and \"200-240cm\". Perhaps we would like to have two columns for the minimum and maximum wingspan in centimetres?\n",
    "- We have **Weights**  of \"3.5-5kg (male); 4-7kg (female)\" and \"10-14g\". Perhaps we would like to convert this into four columns: minimum/maximum Male/Female weight in grammes? But can we assume that the weight range of the Aquatic warbler applies both to Males and Females?\n",
    "\n",
    "We can see why data scientists estimate that about 80% of the time spent on a data science project is involved in data cleaning. It is important to record the steps that you have undertaken in data cleaning, and also important to keep a copy of the original, raw, data, so that your cleaning can be reproduced or improved upon.\n",
    "\n",
    "**Exercise 10: (Optional)** Write down a scheme for how you want to clean the data frame you have collected in Exercise 09. Implement your scheme using regex substitution. Visualise your cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The last FDS lab\n",
    "\n",
    "This was the last FDS lab. We hope you enjoyed it! If you have any comments on the labs or the organisation of the lab sessions, please [email David](mailto:david.c.sterratt@ed.ac.uk?subject=Labs).\n",
    "\n",
    "![Disappearing cheshire cat. Public domain, downloaded from https://www.alice-in-wonderland.net](src/alice-in-wonderland-cheshire-cat-vanishing.jpg)\n",
    "The Cheshire cat disappears in *Alice in Wonderland*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
